FIRST MAPREDUCE PROGRAMM

Mapper:
Read each line of the input file.
Extract the customer ID, points earned, and points redeemed.
Emit key-value pairs where the key is the customer ID and the value is a custom object containing points earned and points redeemed.

Combiner:
Aggregate the points earned and points redeemed for each customer locally on each mapper node.
Emit the same key-value pairs as the mapper, but with aggregated points earned and points redeemed.

Reducer:
Receive the key-value pairs emitted by the mapper and combiner.
Aggregate the points earned and points redeemed for each customer across all mapper nodes.
Output the total points earned and total points redeemed for each customer.


PointsWritable is a custom class that encapsulates points earned and points redeemed.
PointsMapper reads input lines, parses them, and emits key-value pairs with the customer ID as the key and a PointsWritable object as the value.
PointsCombiner aggregates points earned and points redeemed locally on each mapper node.
PointsReducer aggregates points earned and points redeemed across all mapper nodes and outputs the total for each customer.

Compile these Java files, create a JAR file, and then you can run the MapReduce program using Hadoop. Make sure your input file is in HDFS. You can run the program with the following command:

hadoop jar PointsCalculator.jar input_directory output_directory

Replace PointsCalculator.jar with the name of your JAR file, input_directory with the directory containing your input file, and output_directory with the directory where you want to store the output.

CAN I JUST USE THE NAME OF CLASS LIKE "TOKENIZERMAPPER, INTSUMREDUCER, INTSUMCOMBINER"????

Is The "FileContext"-Class necessary? !!!!!!!!!!!!!!!!!

SECOND MAPREDUCE PROGRAMM

Mapper:
Read each line of the input file.
Extract the customer ID and total points earned.
Classify customers into three groups based on their total points earned.
Emit key-value pairs where the key is the group identifier and the value is a custom object containing total points earned and points redeemed.

Combiner :
Aggregate the total points earned and points redeemed for each group locally on each mapper node.
Emit the same key-value pairs as the mapper, but with aggregated total points earned and points redeemed.

Reducer:
Receive the key-value pairs emitted by the mapper and combiner.
Aggregate the total points earned and points redeemed for each group across all mapper nodes.
Calculate the number of customers in each group, total points earned by the group, and the percentage of points redeemed to points earned.
Output these statistics for each group.

PointsGroup is a custom class that encapsulates total points earned and points redeemed for each customer.
PointsMapper reads input lines, parses them, classifies customers into three groups based on their total points earned, and emits key-value pairs with the group identifier as the key and a PointsGroup object as the value.
PointsReducer aggregates total points earned and points redeemed for each group and calculates the required statistics.
PointsCombiner is a new class added as a combiner. It performs local aggregation of points earned and points redeemed within each mapper node before sending the data to the reducer. It works similarly to the reducer, but it operates locally on each mapper's intermediate key-value pairs.

While PointsWritable is used to store points earned and points redeemed for each individual customer,
PointsGroup is used to store aggregated points earned and points redeemed for a group of customers.

HDFS DIRECTORIES

You can define three directories in HDFS for your MapReduce job: one for input, one for intermediate results, and one for final results. Here's how you can do it:

Input Directory: This directory will contain the input data for your MapReduce job.

Intermediate Directory: This directory will store the intermediate results generated by the mappers before they are sent to the reducers.

Output Directory: This directory will store the final output generated by the reducers.
